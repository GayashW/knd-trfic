name: Kandy Traffic Intelligence

on:
  push:
    branches: [ main ]
  # This allows the "Start Live Scrape" button in index.html to work
  repository_dispatch:
    types: [trigger-scrape]
  # Manual trigger via GitHub UI
  workflow_dispatch:
  # Automated schedule (every 30 minutes)
  schedule:
    - cron: '*/30 * * * *'

jobs:
  run-scraper:
    runs-on: ubuntu-latest
    # Grants the bot permission to commit the CSV back to the repo
    permissions:
      contents: write
      
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          # Necessary for the bot to have permission to push changes
          persist-credentials: true

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          # Install Playwright browser and system-level dependencies for Ubuntu
          pip install playwright
          playwright install chromium --with-deps
          # Extra system libs often needed for headless browsers on Linux
          sudo apt-get update && sudo apt-get install -y libgbm1 libgl1

      - name: Run Scraper
        run: python main.py

      - name: Update Repository
        run: |
          # Configure Git Bot Identity
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          
          # Only proceed if the CSV file was updated or created
          if [ -f kandy_od_traffic.csv ]; then
            git add kandy_od_traffic.csv
            # Commit only if there are changes to avoid workflow failure
            git diff --quiet && git diff --staged --quiet || (git commit -m "Live Traffic Sync: $(date)" && git push)
          else
            echo "No data file found to commit."
          fi
